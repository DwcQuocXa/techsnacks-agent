# [#TechSnack 05] | Langfuse ‚Äì observability cho LLM app ch·∫°y production

L·∫ßn s·ªë techsnack 03, m√¨nh c√≥ vi·∫øt v·ªÅ Langchain nh∆∞ 1 framework ƒë·ªÉ build LLM app. Nh∆∞ng l√∫c l√™n production th√¨ c√¢u h·ªèi quen thu·ªôc l·∫°i quay v·ªÅ: **"User k√™u AI tr·∫£ l·ªùi ngu, m√¨nh debug ki·ªÉu g√¨?"**

H√¥m nay th·ª≠ b√†n nhanh v·ªÅ **Langfuse** - m·ªôt open source platform kh√° m·∫°nh cho LLM observability + evaluation + prompt management.

## Langfuse gi·∫£i quy·∫øt b√†i to√°n g√¨?

V·ªÅ b·∫£n ch·∫•t, Langfuse gi√∫p ae "nh√¨n xuy√™n" to√†n b·ªô lifecycle c·ªßa m·ªôt LLM call: input, output, tool call, retry, latency, cost,‚Ä¶ ƒë·ªÅu ƒë∆∞·ª£c trace l·∫°i.

Thay v√¨ ch·ªâ log v√†i d√≤ng text, b·∫°n c√≥ m·ªôt trace tree c·ªßa c·∫£ flow: user query ‚Üí RAG retrieval ‚Üí LLM ‚Üí tools ‚Üí final answer.

T·ª´ ƒë√≥ debug d·ªÖ h∆°n, optimize prompt d·ªÖ h∆°n, v√† bi·∫øt ch√≠nh x√°c ti·ªÅn ƒëang "ch·∫£y" v√†o model n√†o.

## Langfuse c√≥ g√¨ ƒë√°ng ch√∫ √Ω?

- Open source (MIT), self-host b·∫±ng Docker r·∫•t th·∫≥ng, d√πng chung codebase v·ªõi Langfuse Cloud.
- Backend m·ªõi (v3) d√πng ClickHouse n√™n scale t·ªët cho l∆∞·ª£ng trace l·ªõn.
- C√≥ SDK cho Python, TypeScript/JavaScript, integrate ngon v·ªõi LangChain, OpenAI API, v.v.
- H·ªó tr·ª£ evaluation (LLM-as-a-judge, metric custom), prompt versioning, dataset cho A/B test.

N√≥i ng·∫Øn g·ªçn: kh√¥ng ph·∫£i ch·ªâ log viewer, m√† l√† m·ªôt LLM engineering platform ƒë·ªÉ c·∫£ team c√πng l√†m vi·ªác tr√™n c√πng data.

## M·ªôt v√≠ d·ª• d√πng Langfuse r·∫•t th·ª±c t·∫ø

Gi·∫£ s·ª≠ ae build m·ªôt RAG chatbot cho n·ªôi b·ªô c√¥ng ty:

Backend Node.js/Express g·ªçi OpenAI API + vector DB.

B·∫°n c√†i SDK Langfuse, wrap m·ªói request th√†nh m·ªôt trace:

- observation cho b∆∞·ªõc semantic search
- observation cho m·ªói LLM call
- log th√™m metadata: user_id, tenant, document_id,‚Ä¶

Khi user k√™u: **"Chatbot tr·∫£ l·ªùi sai v·ªÅ policy ngh·ªâ ph√©p."**

B·∫°n v√†o Langfuse:

- filter theo user_id / th·ªùi gian
- m·ªü trace, xem r√µ context n√†o ƒë∆∞·ª£c retrieve, prompt version n√†o ƒë∆∞·ª£c d√πng, model tr·∫£ ra text g√¨

Th·∫•y v·∫•n ƒë·ªÅ l√† chunk policy b·ªã thi·∫øu ‚Üí b·∫°n:

- fix pipeline chunking ho·∫∑c index l·∫°i
- ch·ªânh prompt trong Langfuse UI (t·∫°o prompt version m·ªõi)
- deploy prompt m·ªõi kh√¥ng c·∫ßn redeploy backend.

Song song, b·∫°n setup evaluation rule: m·ªçi c√¢u tr·∫£ l·ªùi v·ªÅ "policy" s·∫Ω ƒë∆∞·ª£c LLM ch·∫•m score "correctness". Dashboard cho th·∫•y sau khi ƒë·ªïi prompt, score trung b√¨nh tƒÉng, cost/req gi·ªØ nguy√™n.

## V√¨ sao dev n√™n quan t√¢m?

- ‚úÖ Debug LLM app nhanh h∆°n r·∫•t nhi·ªÅu so v·ªõi grep log th√¥.
- ‚úÖ C√≥ c∆° s·ªü d·ªØ li·ªáu th·ª±c t·∫ø ƒë·ªÉ optimize: latency, cost, quality.
- ‚úÖ Ph√π h·ª£p cho c·∫£ side project (self-host free) l·∫´n production l·ªõn (enterprise feature).

## Gi·ªù ƒë·∫øn l∆∞·ª£t ae:

- Ae ƒëang trace LLM app b·∫±ng g√¨?
- C√≥ t√¨nh hu·ªëng production n√†o khi·∫øn ae th·∫•y c·∫ßn m·ªôt tool ki·ªÉu Langfuse kh√¥ng?
- Cmt chia s·∫ª ƒë·ªÉ m·ªçi ng∆∞·ªùi c√πng b√†n c√°ch quan s√°t v√† t·ªëi ∆∞u LLM app nh√©.

**Happy coding!! üòÅ**


